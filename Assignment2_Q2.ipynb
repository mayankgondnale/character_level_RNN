{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163783\n",
      "Total Unique Characters (vocabulary size) :  61\n"
     ]
    }
   ],
   "source": [
    "# Read text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read().lower()\n",
    "\n",
    "# Create mapping of unique characters to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# Print out the number of total characters and total unique characters\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Unique Characters (vocabulary size) : \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '?': 27, '@': 28, '[': 29, ']': 30, '_': 31, 'a': 32, 'b': 33, 'c': 34, 'd': 35, 'e': 36, 'f': 37, 'g': 38, 'h': 39, 'i': 40, 'j': 41, 'k': 42, 'l': 43, 'm': 44, 'n': 45, 'o': 46, 'p': 47, 'q': 48, 'r': 49, 's': 50, 't': 51, 'u': 52, 'v': 53, 'w': 54, 'x': 55, 'y': 56, 'z': 57, '»': 58, '¿': 59, 'ï': 60}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163773\n"
     ]
    }
   ],
   "source": [
    "# Create input and output dataset and encode them as integers\n",
    "# Use step size = 10\n",
    "\n",
    "seq_length = 10\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input dataX to be in the format [samples, time steps, features] (as expected by KERAS)\n",
    "\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize X to make learning easy\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 300)               362400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 58)                17458     \n",
      "=================================================================\n",
      "Total params: 470,158\n",
      "Trainable params: 470,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model with 300 hidden neurons\n",
    "# 2 layers in total each with 300 neurons\n",
    "# Layer 1 is an LSTM layer\n",
    "# Layer 2 is a dense layer with tanh activation\n",
    "# Finally used softmax for classification\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(300, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile model and use Nadam optimizer\n",
    "# Tried RMSprop, Adam and Nadam, and found that Nadam gives best loss and accuracy)\n",
    "\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "optimizer = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoints for storing weights at different epochs and loss values\n",
    "\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "163773/163773 [==============================] - 150s 914us/step - loss: 2.8751 - acc: 0.2093\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.87511, saving model to weights-improvement-01-2.8751.hdf5\n",
      "Epoch 2/150\n",
      "163773/163773 [==============================] - 150s 915us/step - loss: 2.7321 - acc: 0.2430\n",
      "\n",
      "Epoch 00002: loss improved from 2.87511 to 2.73213, saving model to weights-improvement-02-2.7321.hdf5\n",
      "Epoch 3/150\n",
      "163773/163773 [==============================] - 148s 905us/step - loss: 2.6751 - acc: 0.2539\n",
      "\n",
      "Epoch 00003: loss improved from 2.73213 to 2.67507, saving model to weights-improvement-03-2.6751.hdf5\n",
      "Epoch 4/150\n",
      "163773/163773 [==============================] - 150s 918us/step - loss: 2.6226 - acc: 0.2614\n",
      "\n",
      "Epoch 00004: loss improved from 2.67507 to 2.62258, saving model to weights-improvement-04-2.6226.hdf5\n",
      "Epoch 5/150\n",
      "163773/163773 [==============================] - 146s 893us/step - loss: 2.5736 - acc: 0.2718\n",
      "\n",
      "Epoch 00005: loss improved from 2.62258 to 2.57357, saving model to weights-improvement-05-2.5736.hdf5\n",
      "Epoch 6/150\n",
      "163773/163773 [==============================] - 147s 898us/step - loss: 2.5297 - acc: 0.2831\n",
      "\n",
      "Epoch 00006: loss improved from 2.57357 to 2.52969, saving model to weights-improvement-06-2.5297.hdf5\n",
      "Epoch 7/150\n",
      "163773/163773 [==============================] - 151s 924us/step - loss: 2.4808 - acc: 0.2952\n",
      "\n",
      "Epoch 00007: loss improved from 2.52969 to 2.48076, saving model to weights-improvement-07-2.4808.hdf5\n",
      "Epoch 8/150\n",
      "163773/163773 [==============================] - 152s 931us/step - loss: 2.4332 - acc: 0.3070\n",
      "\n",
      "Epoch 00008: loss improved from 2.48076 to 2.43319, saving model to weights-improvement-08-2.4332.hdf5\n",
      "Epoch 9/150\n",
      "163773/163773 [==============================] - 150s 913us/step - loss: 2.3862 - acc: 0.3207\n",
      "\n",
      "Epoch 00009: loss improved from 2.43319 to 2.38623, saving model to weights-improvement-09-2.3862.hdf5\n",
      "Epoch 10/150\n",
      "163773/163773 [==============================] - 149s 913us/step - loss: 2.3374 - acc: 0.3344\n",
      "\n",
      "Epoch 00010: loss improved from 2.38623 to 2.33744, saving model to weights-improvement-10-2.3374.hdf5\n",
      "Epoch 11/150\n",
      "163773/163773 [==============================] - 147s 900us/step - loss: 2.2923 - acc: 0.3470\n",
      "\n",
      "Epoch 00011: loss improved from 2.33744 to 2.29225, saving model to weights-improvement-11-2.2923.hdf5\n",
      "Epoch 12/150\n",
      "163773/163773 [==============================] - 148s 905us/step - loss: 2.2495 - acc: 0.3571\n",
      "\n",
      "Epoch 00012: loss improved from 2.29225 to 2.24952, saving model to weights-improvement-12-2.2495.hdf5\n",
      "Epoch 13/150\n",
      "163773/163773 [==============================] - 147s 897us/step - loss: 2.2064 - acc: 0.3686\n",
      "\n",
      "Epoch 00013: loss improved from 2.24952 to 2.20639, saving model to weights-improvement-13-2.2064.hdf5\n",
      "Epoch 14/150\n",
      "163773/163773 [==============================] - 153s 932us/step - loss: 2.1675 - acc: 0.3787\n",
      "\n",
      "Epoch 00014: loss improved from 2.20639 to 2.16747, saving model to weights-improvement-14-2.1675.hdf5\n",
      "Epoch 15/150\n",
      "163773/163773 [==============================] - 151s 923us/step - loss: 2.1294 - acc: 0.3878\n",
      "\n",
      "Epoch 00015: loss improved from 2.16747 to 2.12941, saving model to weights-improvement-15-2.1294.hdf5\n",
      "Epoch 16/150\n",
      "163773/163773 [==============================] - 151s 922us/step - loss: 2.0951 - acc: 0.3975\n",
      "\n",
      "Epoch 00016: loss improved from 2.12941 to 2.09509, saving model to weights-improvement-16-2.0951.hdf5\n",
      "Epoch 17/150\n",
      "163773/163773 [==============================] - 153s 931us/step - loss: 2.0593 - acc: 0.4064\n",
      "\n",
      "Epoch 00017: loss improved from 2.09509 to 2.05927, saving model to weights-improvement-17-2.0593.hdf5\n",
      "Epoch 18/150\n",
      "163773/163773 [==============================] - 151s 924us/step - loss: 2.0256 - acc: 0.4148\n",
      "\n",
      "Epoch 00018: loss improved from 2.05927 to 2.02556, saving model to weights-improvement-18-2.0256.hdf5\n",
      "Epoch 19/150\n",
      "163773/163773 [==============================] - 151s 924us/step - loss: 1.9942 - acc: 0.4231\n",
      "\n",
      "Epoch 00019: loss improved from 2.02556 to 1.99421, saving model to weights-improvement-19-1.9942.hdf5\n",
      "Epoch 20/150\n",
      "163773/163773 [==============================] - 152s 929us/step - loss: 1.9635 - acc: 0.4314\n",
      "\n",
      "Epoch 00020: loss improved from 1.99421 to 1.96345, saving model to weights-improvement-20-1.9635.hdf5\n",
      "Epoch 21/150\n",
      "163773/163773 [==============================] - 151s 925us/step - loss: 1.9360 - acc: 0.4375\n",
      "\n",
      "Epoch 00021: loss improved from 1.96345 to 1.93599, saving model to weights-improvement-21-1.9360.hdf5\n",
      "Epoch 22/150\n",
      "163773/163773 [==============================] - 152s 926us/step - loss: 1.9047 - acc: 0.4452\n",
      "\n",
      "Epoch 00022: loss improved from 1.93599 to 1.90474, saving model to weights-improvement-22-1.9047.hdf5\n",
      "Epoch 23/150\n",
      "163773/163773 [==============================] - 149s 908us/step - loss: 1.8804 - acc: 0.4515\n",
      "\n",
      "Epoch 00023: loss improved from 1.90474 to 1.88043, saving model to weights-improvement-23-1.8804.hdf5\n",
      "Epoch 24/150\n",
      "163773/163773 [==============================] - 150s 913us/step - loss: 1.8527 - acc: 0.4587\n",
      "\n",
      "Epoch 00024: loss improved from 1.88043 to 1.85267, saving model to weights-improvement-24-1.8527.hdf5\n",
      "Epoch 25/150\n",
      "163773/163773 [==============================] - 149s 908us/step - loss: 1.8240 - acc: 0.4642\n",
      "\n",
      "Epoch 00025: loss improved from 1.85267 to 1.82397, saving model to weights-improvement-25-1.8240.hdf5\n",
      "Epoch 26/150\n",
      "163773/163773 [==============================] - 149s 911us/step - loss: 1.8002 - acc: 0.4710\n",
      "\n",
      "Epoch 00026: loss improved from 1.82397 to 1.80022, saving model to weights-improvement-26-1.8002.hdf5\n",
      "Epoch 27/150\n",
      "163773/163773 [==============================] - 150s 913us/step - loss: 1.7730 - acc: 0.4781\n",
      "\n",
      "Epoch 00027: loss improved from 1.80022 to 1.77296, saving model to weights-improvement-27-1.7730.hdf5\n",
      "Epoch 28/150\n",
      "163773/163773 [==============================] - 149s 909us/step - loss: 1.7531 - acc: 0.4830\n",
      "\n",
      "Epoch 00028: loss improved from 1.77296 to 1.75306, saving model to weights-improvement-28-1.7531.hdf5\n",
      "Epoch 29/150\n",
      "163773/163773 [==============================] - 149s 911us/step - loss: 1.7275 - acc: 0.4898\n",
      "\n",
      "Epoch 00029: loss improved from 1.75306 to 1.72747, saving model to weights-improvement-29-1.7275.hdf5\n",
      "Epoch 30/150\n",
      "163773/163773 [==============================] - 149s 912us/step - loss: 1.7052 - acc: 0.4940\n",
      "\n",
      "Epoch 00030: loss improved from 1.72747 to 1.70518, saving model to weights-improvement-30-1.7052.hdf5\n",
      "Epoch 31/150\n",
      "163773/163773 [==============================] - 150s 913us/step - loss: 1.6848 - acc: 0.4991\n",
      "\n",
      "Epoch 00031: loss improved from 1.70518 to 1.68484, saving model to weights-improvement-31-1.6848.hdf5\n",
      "Epoch 32/150\n",
      "163773/163773 [==============================] - 150s 917us/step - loss: 1.6642 - acc: 0.5039\n",
      "\n",
      "Epoch 00032: loss improved from 1.68484 to 1.66422, saving model to weights-improvement-32-1.6642.hdf5\n",
      "Epoch 33/150\n",
      "163773/163773 [==============================] - 149s 913us/step - loss: 1.6424 - acc: 0.5113\n",
      "\n",
      "Epoch 00033: loss improved from 1.66422 to 1.64244, saving model to weights-improvement-33-1.6424.hdf5\n",
      "Epoch 34/150\n",
      "163773/163773 [==============================] - 150s 913us/step - loss: 1.6245 - acc: 0.5154\n",
      "\n",
      "Epoch 00034: loss improved from 1.64244 to 1.62453, saving model to weights-improvement-34-1.6245.hdf5\n",
      "Epoch 35/150\n",
      "163773/163773 [==============================] - 150s 914us/step - loss: 1.6057 - acc: 0.5207\n",
      "\n",
      "Epoch 00035: loss improved from 1.62453 to 1.60573, saving model to weights-improvement-35-1.6057.hdf5\n",
      "Epoch 36/150\n",
      "163773/163773 [==============================] - 150s 918us/step - loss: 1.5877 - acc: 0.5247\n",
      "\n",
      "Epoch 00036: loss improved from 1.60573 to 1.58768, saving model to weights-improvement-36-1.5877.hdf5\n",
      "Epoch 37/150\n",
      "163773/163773 [==============================] - 150s 918us/step - loss: 1.5697 - acc: 0.5294\n",
      "\n",
      "Epoch 00037: loss improved from 1.58768 to 1.56971, saving model to weights-improvement-37-1.5697.hdf5\n",
      "Epoch 38/150\n",
      "163773/163773 [==============================] - 151s 922us/step - loss: 1.5536 - acc: 0.5322\n",
      "\n",
      "Epoch 00038: loss improved from 1.56971 to 1.55357, saving model to weights-improvement-38-1.5536.hdf5\n",
      "Epoch 39/150\n",
      "163773/163773 [==============================] - 151s 920us/step - loss: 1.5378 - acc: 0.5370\n",
      "\n",
      "Epoch 00039: loss improved from 1.55357 to 1.53777, saving model to weights-improvement-39-1.5378.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/150\n",
      "163773/163773 [==============================] - 148s 904us/step - loss: 1.5246 - acc: 0.5401\n",
      "\n",
      "Epoch 00040: loss improved from 1.53777 to 1.52462, saving model to weights-improvement-40-1.5246.hdf5\n",
      "Epoch 41/150\n",
      "163773/163773 [==============================] - 149s 908us/step - loss: 1.5079 - acc: 0.5442\n",
      "\n",
      "Epoch 00041: loss improved from 1.52462 to 1.50791, saving model to weights-improvement-41-1.5079.hdf5\n",
      "Epoch 42/150\n",
      "163773/163773 [==============================] - 149s 907us/step - loss: 1.4938 - acc: 0.5473\n",
      "\n",
      "Epoch 00042: loss improved from 1.50791 to 1.49382, saving model to weights-improvement-42-1.4938.hdf5\n",
      "Epoch 43/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 1.4775 - acc: 0.5530\n",
      "\n",
      "Epoch 00043: loss improved from 1.49382 to 1.47749, saving model to weights-improvement-43-1.4775.hdf5\n",
      "Epoch 44/150\n",
      "163773/163773 [==============================] - 146s 892us/step - loss: 1.4608 - acc: 0.5565\n",
      "\n",
      "Epoch 00044: loss improved from 1.47749 to 1.46083, saving model to weights-improvement-44-1.4608.hdf5\n",
      "Epoch 45/150\n",
      "163773/163773 [==============================] - 146s 893us/step - loss: 1.4512 - acc: 0.5590\n",
      "\n",
      "Epoch 00045: loss improved from 1.46083 to 1.45121, saving model to weights-improvement-45-1.4512.hdf5\n",
      "Epoch 46/150\n",
      "163773/163773 [==============================] - 148s 902us/step - loss: 1.4361 - acc: 0.5632\n",
      "\n",
      "Epoch 00046: loss improved from 1.45121 to 1.43613, saving model to weights-improvement-46-1.4361.hdf5\n",
      "Epoch 47/150\n",
      "163773/163773 [==============================] - 147s 899us/step - loss: 1.4216 - acc: 0.5665\n",
      "\n",
      "Epoch 00047: loss improved from 1.43613 to 1.42164, saving model to weights-improvement-47-1.4216.hdf5\n",
      "Epoch 48/150\n",
      "163773/163773 [==============================] - 148s 905us/step - loss: 1.4155 - acc: 0.5686\n",
      "\n",
      "Epoch 00048: loss improved from 1.42164 to 1.41549, saving model to weights-improvement-48-1.4155.hdf5\n",
      "Epoch 49/150\n",
      "163773/163773 [==============================] - 147s 896us/step - loss: 1.3974 - acc: 0.5723\n",
      "\n",
      "Epoch 00049: loss improved from 1.41549 to 1.39736, saving model to weights-improvement-49-1.3974.hdf5\n",
      "Epoch 50/150\n",
      "163773/163773 [==============================] - 147s 899us/step - loss: 1.3876 - acc: 0.5752\n",
      "\n",
      "Epoch 00050: loss improved from 1.39736 to 1.38759, saving model to weights-improvement-50-1.3876.hdf5\n",
      "Epoch 51/150\n",
      "163773/163773 [==============================] - 147s 898us/step - loss: 1.3742 - acc: 0.5792\n",
      "\n",
      "Epoch 00051: loss improved from 1.38759 to 1.37424, saving model to weights-improvement-51-1.3742.hdf5\n",
      "Epoch 52/150\n",
      "163773/163773 [==============================] - 147s 899us/step - loss: 1.3633 - acc: 0.5818\n",
      "\n",
      "Epoch 00052: loss improved from 1.37424 to 1.36328, saving model to weights-improvement-52-1.3633.hdf5\n",
      "Epoch 53/150\n",
      "163773/163773 [==============================] - 147s 899us/step - loss: 1.3573 - acc: 0.5838\n",
      "\n",
      "Epoch 00053: loss improved from 1.36328 to 1.35731, saving model to weights-improvement-53-1.3573.hdf5\n",
      "Epoch 54/150\n",
      "163773/163773 [==============================] - 148s 903us/step - loss: 1.3405 - acc: 0.5883\n",
      "\n",
      "Epoch 00054: loss improved from 1.35731 to 1.34053, saving model to weights-improvement-54-1.3405.hdf5\n",
      "Epoch 55/150\n",
      "163773/163773 [==============================] - 148s 904us/step - loss: 1.3370 - acc: 0.5878\n",
      "\n",
      "Epoch 00055: loss improved from 1.34053 to 1.33697, saving model to weights-improvement-55-1.3370.hdf5\n",
      "Epoch 56/150\n",
      "163773/163773 [==============================] - 150s 915us/step - loss: 1.3244 - acc: 0.5927\n",
      "\n",
      "Epoch 00056: loss improved from 1.33697 to 1.32435, saving model to weights-improvement-56-1.3244.hdf5\n",
      "Epoch 57/150\n",
      "163773/163773 [==============================] - 149s 909us/step - loss: 1.3114 - acc: 0.5961\n",
      "\n",
      "Epoch 00057: loss improved from 1.32435 to 1.31144, saving model to weights-improvement-57-1.3114.hdf5\n",
      "Epoch 58/150\n",
      "163773/163773 [==============================] - 150s 916us/step - loss: 1.3059 - acc: 0.5965\n",
      "\n",
      "Epoch 00058: loss improved from 1.31144 to 1.30589, saving model to weights-improvement-58-1.3059.hdf5\n",
      "Epoch 59/150\n",
      "163773/163773 [==============================] - 149s 912us/step - loss: 1.2962 - acc: 0.6003\n",
      "\n",
      "Epoch 00059: loss improved from 1.30589 to 1.29623, saving model to weights-improvement-59-1.2962.hdf5\n",
      "Epoch 60/150\n",
      "163773/163773 [==============================] - 150s 918us/step - loss: 1.2885 - acc: 0.6013\n",
      "\n",
      "Epoch 00060: loss improved from 1.29623 to 1.28848, saving model to weights-improvement-60-1.2885.hdf5\n",
      "Epoch 61/150\n",
      "163773/163773 [==============================] - 152s 931us/step - loss: 1.2763 - acc: 0.6035\n",
      "\n",
      "Epoch 00061: loss improved from 1.28848 to 1.27632, saving model to weights-improvement-61-1.2763.hdf5\n",
      "Epoch 62/150\n",
      "163773/163773 [==============================] - 152s 929us/step - loss: 1.2722 - acc: 0.6063\n",
      "\n",
      "Epoch 00062: loss improved from 1.27632 to 1.27221, saving model to weights-improvement-62-1.2722.hdf5\n",
      "Epoch 63/150\n",
      "163773/163773 [==============================] - 150s 916us/step - loss: 1.2630 - acc: 0.6086\n",
      "\n",
      "Epoch 00063: loss improved from 1.27221 to 1.26296, saving model to weights-improvement-63-1.2630.hdf5\n",
      "Epoch 64/150\n",
      "163773/163773 [==============================] - 149s 912us/step - loss: 1.2557 - acc: 0.6102\n",
      "\n",
      "Epoch 00064: loss improved from 1.26296 to 1.25572, saving model to weights-improvement-64-1.2557.hdf5\n",
      "Epoch 65/150\n",
      "163773/163773 [==============================] - 149s 908us/step - loss: 1.2481 - acc: 0.6128\n",
      "\n",
      "Epoch 00065: loss improved from 1.25572 to 1.24809, saving model to weights-improvement-65-1.2481.hdf5\n",
      "Epoch 66/150\n",
      "163773/163773 [==============================] - 149s 911us/step - loss: 1.2403 - acc: 0.6142\n",
      "\n",
      "Epoch 00066: loss improved from 1.24809 to 1.24026, saving model to weights-improvement-66-1.2403.hdf5\n",
      "Epoch 67/150\n",
      "163773/163773 [==============================] - 149s 911us/step - loss: 1.2324 - acc: 0.6168\n",
      "\n",
      "Epoch 00067: loss improved from 1.24026 to 1.23245, saving model to weights-improvement-67-1.2324.hdf5\n",
      "Epoch 68/150\n",
      "163773/163773 [==============================] - 150s 916us/step - loss: 1.2250 - acc: 0.6187\n",
      "\n",
      "Epoch 00068: loss improved from 1.23245 to 1.22505, saving model to weights-improvement-68-1.2250.hdf5\n",
      "Epoch 69/150\n",
      "163773/163773 [==============================] - 150s 915us/step - loss: 1.2152 - acc: 0.6216\n",
      "\n",
      "Epoch 00069: loss improved from 1.22505 to 1.21521, saving model to weights-improvement-69-1.2152.hdf5\n",
      "Epoch 70/150\n",
      "163773/163773 [==============================] - 150s 915us/step - loss: 1.2094 - acc: 0.6236\n",
      "\n",
      "Epoch 00070: loss improved from 1.21521 to 1.20935, saving model to weights-improvement-70-1.2094.hdf5\n",
      "Epoch 71/150\n",
      "163773/163773 [==============================] - 150s 915us/step - loss: 1.2016 - acc: 0.6246\n",
      "\n",
      "Epoch 00071: loss improved from 1.20935 to 1.20158, saving model to weights-improvement-71-1.2016.hdf5\n",
      "Epoch 72/150\n",
      "163773/163773 [==============================] - 151s 921us/step - loss: 1.1947 - acc: 0.6278\n",
      "\n",
      "Epoch 00072: loss improved from 1.20158 to 1.19472, saving model to weights-improvement-72-1.1947.hdf5\n",
      "Epoch 73/150\n",
      "163773/163773 [==============================] - 150s 917us/step - loss: 1.1869 - acc: 0.6282\n",
      "\n",
      "Epoch 00073: loss improved from 1.19472 to 1.18686, saving model to weights-improvement-73-1.1869.hdf5\n",
      "Epoch 74/150\n",
      "163773/163773 [==============================] - 151s 919us/step - loss: 1.1847 - acc: 0.6299\n",
      "\n",
      "Epoch 00074: loss improved from 1.18686 to 1.18470, saving model to weights-improvement-74-1.1847.hdf5\n",
      "Epoch 75/150\n",
      "163773/163773 [==============================] - 151s 920us/step - loss: 1.1777 - acc: 0.6312\n",
      "\n",
      "Epoch 00075: loss improved from 1.18470 to 1.17767, saving model to weights-improvement-75-1.1777.hdf5\n",
      "Epoch 76/150\n",
      "163773/163773 [==============================] - 151s 923us/step - loss: 1.1717 - acc: 0.6327\n",
      "\n",
      "Epoch 00076: loss improved from 1.17767 to 1.17168, saving model to weights-improvement-76-1.1717.hdf5\n",
      "Epoch 77/150\n",
      "163773/163773 [==============================] - 151s 922us/step - loss: 1.1645 - acc: 0.6347\n",
      "\n",
      "Epoch 00077: loss improved from 1.17168 to 1.16455, saving model to weights-improvement-77-1.1645.hdf5\n",
      "Epoch 78/150\n",
      "163773/163773 [==============================] - 151s 925us/step - loss: 1.1602 - acc: 0.6365\n",
      "\n",
      "Epoch 00078: loss improved from 1.16455 to 1.16025, saving model to weights-improvement-78-1.1602.hdf5\n",
      "Epoch 79/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163773/163773 [==============================] - 147s 900us/step - loss: 1.1519 - acc: 0.6372\n",
      "\n",
      "Epoch 00079: loss improved from 1.16025 to 1.15187, saving model to weights-improvement-79-1.1519.hdf5\n",
      "Epoch 80/150\n",
      "163773/163773 [==============================] - 148s 902us/step - loss: 1.1474 - acc: 0.6394\n",
      "\n",
      "Epoch 00080: loss improved from 1.15187 to 1.14744, saving model to weights-improvement-80-1.1474.hdf5\n",
      "Epoch 81/150\n",
      "163773/163773 [==============================] - 147s 900us/step - loss: 1.1418 - acc: 0.6414\n",
      "\n",
      "Epoch 00081: loss improved from 1.14744 to 1.14177, saving model to weights-improvement-81-1.1418.hdf5\n",
      "Epoch 82/150\n",
      "163773/163773 [==============================] - 148s 904us/step - loss: 1.1385 - acc: 0.6425\n",
      "\n",
      "Epoch 00082: loss improved from 1.14177 to 1.13851, saving model to weights-improvement-82-1.1385.hdf5\n",
      "Epoch 83/150\n",
      "163773/163773 [==============================] - 148s 904us/step - loss: 1.1355 - acc: 0.6438\n",
      "\n",
      "Epoch 00083: loss improved from 1.13851 to 1.13549, saving model to weights-improvement-83-1.1355.hdf5\n",
      "Epoch 84/150\n",
      "163773/163773 [==============================] - 148s 904us/step - loss: 1.1291 - acc: 0.6436\n",
      "\n",
      "Epoch 00084: loss improved from 1.13549 to 1.12911, saving model to weights-improvement-84-1.1291.hdf5\n",
      "Epoch 85/150\n",
      "163773/163773 [==============================] - 147s 898us/step - loss: 1.1237 - acc: 0.6456\n",
      "\n",
      "Epoch 00085: loss improved from 1.12911 to 1.12369, saving model to weights-improvement-85-1.1237.hdf5\n",
      "Epoch 86/150\n",
      "163773/163773 [==============================] - 147s 900us/step - loss: 1.1159 - acc: 0.6478\n",
      "\n",
      "Epoch 00086: loss improved from 1.12369 to 1.11587, saving model to weights-improvement-86-1.1159.hdf5\n",
      "Epoch 87/150\n",
      "163773/163773 [==============================] - 148s 901us/step - loss: 1.1103 - acc: 0.6492\n",
      "\n",
      "Epoch 00087: loss improved from 1.11587 to 1.11031, saving model to weights-improvement-87-1.1103.hdf5\n",
      "Epoch 88/150\n",
      "163773/163773 [==============================] - 148s 902us/step - loss: 1.1073 - acc: 0.6497\n",
      "\n",
      "Epoch 00088: loss improved from 1.11031 to 1.10726, saving model to weights-improvement-88-1.1073.hdf5\n",
      "Epoch 89/150\n",
      "163773/163773 [==============================] - 148s 903us/step - loss: 1.1021 - acc: 0.6523\n",
      "\n",
      "Epoch 00089: loss improved from 1.10726 to 1.10209, saving model to weights-improvement-89-1.1021.hdf5\n",
      "Epoch 90/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 1.0957 - acc: 0.6536\n",
      "\n",
      "Epoch 00090: loss improved from 1.10209 to 1.09566, saving model to weights-improvement-90-1.0957.hdf5\n",
      "Epoch 91/150\n",
      "163773/163773 [==============================] - 148s 903us/step - loss: 1.0931 - acc: 0.6545\n",
      "\n",
      "Epoch 00091: loss improved from 1.09566 to 1.09315, saving model to weights-improvement-91-1.0931.hdf5\n",
      "Epoch 92/150\n",
      "163773/163773 [==============================] - 148s 904us/step - loss: 1.0892 - acc: 0.6546\n",
      "\n",
      "Epoch 00092: loss improved from 1.09315 to 1.08922, saving model to weights-improvement-92-1.0892.hdf5\n",
      "Epoch 93/150\n",
      "163773/163773 [==============================] - 148s 902us/step - loss: 1.0864 - acc: 0.6558\n",
      "\n",
      "Epoch 00093: loss improved from 1.08922 to 1.08640, saving model to weights-improvement-93-1.0864.hdf5\n",
      "Epoch 94/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 1.0789 - acc: 0.6578\n",
      "\n",
      "Epoch 00094: loss improved from 1.08640 to 1.07890, saving model to weights-improvement-94-1.0789.hdf5\n",
      "Epoch 95/150\n",
      "163773/163773 [==============================] - 148s 905us/step - loss: 1.0779 - acc: 0.6585\n",
      "\n",
      "Epoch 00095: loss improved from 1.07890 to 1.07791, saving model to weights-improvement-95-1.0779.hdf5\n",
      "Epoch 96/150\n",
      "163773/163773 [==============================] - 148s 905us/step - loss: 1.0703 - acc: 0.6594\n",
      "\n",
      "Epoch 00096: loss improved from 1.07791 to 1.07032, saving model to weights-improvement-96-1.0703.hdf5\n",
      "Epoch 97/150\n",
      "163773/163773 [==============================] - 148s 903us/step - loss: 1.0671 - acc: 0.6605\n",
      "\n",
      "Epoch 00097: loss improved from 1.07032 to 1.06706, saving model to weights-improvement-97-1.0671.hdf5\n",
      "Epoch 98/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 1.0659 - acc: 0.6621\n",
      "\n",
      "Epoch 00098: loss improved from 1.06706 to 1.06592, saving model to weights-improvement-98-1.0659.hdf5\n",
      "Epoch 99/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 1.0578 - acc: 0.6633\n",
      "\n",
      "Epoch 00099: loss improved from 1.06592 to 1.05780, saving model to weights-improvement-99-1.0578.hdf5\n",
      "Epoch 100/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 1.0594 - acc: 0.6629\n",
      "\n",
      "Epoch 00100: loss did not improve\n",
      "Epoch 101/150\n",
      "163773/163773 [==============================] - 149s 908us/step - loss: 1.0466 - acc: 0.6658\n",
      "\n",
      "Epoch 00101: loss improved from 1.05780 to 1.04663, saving model to weights-improvement-101-1.0466.hdf5\n",
      "Epoch 102/150\n",
      "163773/163773 [==============================] - 149s 908us/step - loss: 1.0451 - acc: 0.6671\n",
      "\n",
      "Epoch 00102: loss improved from 1.04663 to 1.04513, saving model to weights-improvement-102-1.0451.hdf5\n",
      "Epoch 103/150\n",
      "163773/163773 [==============================] - 149s 910us/step - loss: 1.0413 - acc: 0.6690\n",
      "\n",
      "Epoch 00103: loss improved from 1.04513 to 1.04130, saving model to weights-improvement-103-1.0413.hdf5\n",
      "Epoch 104/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 1.0374 - acc: 0.6708\n",
      "\n",
      "Epoch 00104: loss improved from 1.04130 to 1.03735, saving model to weights-improvement-104-1.0374.hdf5\n",
      "Epoch 105/150\n",
      "163773/163773 [==============================] - 149s 909us/step - loss: 1.0381 - acc: 0.6690\n",
      "\n",
      "Epoch 00105: loss did not improve\n",
      "Epoch 106/150\n",
      "163773/163773 [==============================] - 149s 910us/step - loss: 1.0329 - acc: 0.6708\n",
      "\n",
      "Epoch 00106: loss improved from 1.03735 to 1.03289, saving model to weights-improvement-106-1.0329.hdf5\n",
      "Epoch 107/150\n",
      "163773/163773 [==============================] - 150s 913us/step - loss: 1.0319 - acc: 0.6715\n",
      "\n",
      "Epoch 00107: loss improved from 1.03289 to 1.03194, saving model to weights-improvement-107-1.0319.hdf5\n",
      "Epoch 108/150\n",
      "163773/163773 [==============================] - 150s 914us/step - loss: 1.0271 - acc: 0.6731\n",
      "\n",
      "Epoch 00108: loss improved from 1.03194 to 1.02709, saving model to weights-improvement-108-1.0271.hdf5\n",
      "Epoch 109/150\n",
      "163773/163773 [==============================] - 150s 917us/step - loss: 1.0222 - acc: 0.6748\n",
      "\n",
      "Epoch 00109: loss improved from 1.02709 to 1.02219, saving model to weights-improvement-109-1.0222.hdf5\n",
      "Epoch 110/150\n",
      "163773/163773 [==============================] - 150s 918us/step - loss: 1.0250 - acc: 0.6730\n",
      "\n",
      "Epoch 00110: loss did not improve\n",
      "Epoch 111/150\n",
      "163773/163773 [==============================] - 151s 921us/step - loss: 1.0190 - acc: 0.6767\n",
      "\n",
      "Epoch 00111: loss improved from 1.02219 to 1.01902, saving model to weights-improvement-111-1.0190.hdf5\n",
      "Epoch 112/150\n",
      "163773/163773 [==============================] - 150s 919us/step - loss: 1.0150 - acc: 0.6755\n",
      "\n",
      "Epoch 00112: loss improved from 1.01902 to 1.01503, saving model to weights-improvement-112-1.0150.hdf5\n",
      "Epoch 113/150\n",
      "163773/163773 [==============================] - 151s 921us/step - loss: 1.0105 - acc: 0.6782\n",
      "\n",
      "Epoch 00113: loss improved from 1.01503 to 1.01053, saving model to weights-improvement-113-1.0105.hdf5\n",
      "Epoch 114/150\n",
      "163773/163773 [==============================] - 152s 926us/step - loss: 1.0029 - acc: 0.6792\n",
      "\n",
      "Epoch 00114: loss improved from 1.01053 to 1.00292, saving model to weights-improvement-114-1.0029.hdf5\n",
      "Epoch 115/150\n",
      "163773/163773 [==============================] - 151s 920us/step - loss: 1.0067 - acc: 0.6781\n",
      "\n",
      "Epoch 00115: loss did not improve\n",
      "Epoch 116/150\n",
      "163773/163773 [==============================] - 151s 923us/step - loss: 1.0004 - acc: 0.6795\n",
      "\n",
      "Epoch 00116: loss improved from 1.00292 to 1.00043, saving model to weights-improvement-116-1.0004.hdf5\n",
      "Epoch 117/150\n",
      "163773/163773 [==============================] - 151s 921us/step - loss: 0.9958 - acc: 0.6821\n",
      "\n",
      "Epoch 00117: loss improved from 1.00043 to 0.99576, saving model to weights-improvement-117-0.9958.hdf5\n",
      "Epoch 118/150\n",
      "163773/163773 [==============================] - 151s 924us/step - loss: 0.9965 - acc: 0.6812\n",
      "\n",
      "Epoch 00118: loss did not improve\n",
      "Epoch 119/150\n",
      "163773/163773 [==============================] - 148s 905us/step - loss: 0.9917 - acc: 0.6830\n",
      "\n",
      "Epoch 00119: loss improved from 0.99576 to 0.99166, saving model to weights-improvement-119-0.9917.hdf5\n",
      "Epoch 120/150\n",
      "163773/163773 [==============================] - 149s 907us/step - loss: 0.9927 - acc: 0.6822\n",
      "\n",
      "Epoch 00120: loss did not improve\n",
      "Epoch 121/150\n",
      "163773/163773 [==============================] - 149s 907us/step - loss: 0.9844 - acc: 0.6856\n",
      "\n",
      "Epoch 00121: loss improved from 0.99166 to 0.98440, saving model to weights-improvement-121-0.9844.hdf5\n",
      "Epoch 122/150\n",
      "163773/163773 [==============================] - 150s 913us/step - loss: 0.9850 - acc: 0.6854\n",
      "\n",
      "Epoch 00122: loss did not improve\n",
      "Epoch 123/150\n",
      "163773/163773 [==============================] - 149s 911us/step - loss: 0.9775 - acc: 0.6865\n",
      "\n",
      "Epoch 00123: loss improved from 0.98440 to 0.97745, saving model to weights-improvement-123-0.9775.hdf5\n",
      "Epoch 124/150\n",
      "163773/163773 [==============================] - 148s 903us/step - loss: 0.9772 - acc: 0.6883\n",
      "\n",
      "Epoch 00124: loss improved from 0.97745 to 0.97716, saving model to weights-improvement-124-0.9772.hdf5\n",
      "Epoch 125/150\n",
      "163773/163773 [==============================] - 148s 907us/step - loss: 0.9793 - acc: 0.6866\n",
      "\n",
      "Epoch 00125: loss did not improve\n",
      "Epoch 126/150\n",
      "163773/163773 [==============================] - 148s 904us/step - loss: 0.9744 - acc: 0.6876\n",
      "\n",
      "Epoch 00126: loss improved from 0.97716 to 0.97441, saving model to weights-improvement-126-0.9744.hdf5\n",
      "Epoch 127/150\n",
      "163773/163773 [==============================] - 148s 907us/step - loss: 0.9700 - acc: 0.6880\n",
      "\n",
      "Epoch 00127: loss improved from 0.97441 to 0.97000, saving model to weights-improvement-127-0.9700.hdf5\n",
      "Epoch 128/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 0.9714 - acc: 0.6891\n",
      "\n",
      "Epoch 00128: loss did not improve\n",
      "Epoch 129/150\n",
      "163773/163773 [==============================] - 149s 908us/step - loss: 0.9655 - acc: 0.6905\n",
      "\n",
      "Epoch 00129: loss improved from 0.97000 to 0.96546, saving model to weights-improvement-129-0.9655.hdf5\n",
      "Epoch 130/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 0.9635 - acc: 0.6914\n",
      "\n",
      "Epoch 00130: loss improved from 0.96546 to 0.96349, saving model to weights-improvement-130-0.9635.hdf5\n",
      "Epoch 131/150\n",
      "163773/163773 [==============================] - 149s 912us/step - loss: 0.9581 - acc: 0.6912\n",
      "\n",
      "Epoch 00131: loss improved from 0.96349 to 0.95814, saving model to weights-improvement-131-0.9581.hdf5\n",
      "Epoch 132/150\n",
      "163773/163773 [==============================] - 149s 908us/step - loss: 0.9584 - acc: 0.6923\n",
      "\n",
      "Epoch 00132: loss did not improve\n",
      "Epoch 133/150\n",
      "163773/163773 [==============================] - 149s 910us/step - loss: 0.9544 - acc: 0.6928\n",
      "\n",
      "Epoch 00133: loss improved from 0.95814 to 0.95439, saving model to weights-improvement-133-0.9544.hdf5\n",
      "Epoch 134/150\n",
      "163773/163773 [==============================] - 149s 910us/step - loss: 0.9526 - acc: 0.6933\n",
      "\n",
      "Epoch 00134: loss improved from 0.95439 to 0.95260, saving model to weights-improvement-134-0.9526.hdf5\n",
      "Epoch 135/150\n",
      "163773/163773 [==============================] - 150s 914us/step - loss: 0.9522 - acc: 0.6933\n",
      "\n",
      "Epoch 00135: loss improved from 0.95260 to 0.95225, saving model to weights-improvement-135-0.9522.hdf5\n",
      "Epoch 136/150\n",
      "163773/163773 [==============================] - 149s 911us/step - loss: 0.9482 - acc: 0.6939\n",
      "\n",
      "Epoch 00136: loss improved from 0.95225 to 0.94821, saving model to weights-improvement-136-0.9482.hdf5\n",
      "Epoch 137/150\n",
      "163773/163773 [==============================] - 149s 912us/step - loss: 0.9487 - acc: 0.6950\n",
      "\n",
      "Epoch 00137: loss did not improve\n",
      "Epoch 138/150\n",
      "163773/163773 [==============================] - 149s 912us/step - loss: 0.9434 - acc: 0.6971\n",
      "\n",
      "Epoch 00138: loss improved from 0.94821 to 0.94338, saving model to weights-improvement-138-0.9434.hdf5\n",
      "Epoch 139/150\n",
      "163773/163773 [==============================] - 150s 913us/step - loss: 0.9427 - acc: 0.6954\n",
      "\n",
      "Epoch 00139: loss improved from 0.94338 to 0.94275, saving model to weights-improvement-139-0.9427.hdf5\n",
      "Epoch 140/150\n",
      "163773/163773 [==============================] - 148s 906us/step - loss: 0.9418 - acc: 0.6967\n",
      "\n",
      "Epoch 00140: loss improved from 0.94275 to 0.94185, saving model to weights-improvement-140-0.9418.hdf5\n",
      "Epoch 141/150\n",
      "163773/163773 [==============================] - 149s 909us/step - loss: 0.9326 - acc: 0.6996\n",
      "\n",
      "Epoch 00141: loss improved from 0.94185 to 0.93262, saving model to weights-improvement-141-0.9326.hdf5\n",
      "Epoch 142/150\n",
      "163773/163773 [==============================] - 149s 908us/step - loss: 0.9319 - acc: 0.6999\n",
      "\n",
      "Epoch 00142: loss improved from 0.93262 to 0.93192, saving model to weights-improvement-142-0.9319.hdf5\n",
      "Epoch 143/150\n",
      "163773/163773 [==============================] - 150s 917us/step - loss: 0.9320 - acc: 0.6990\n",
      "\n",
      "Epoch 00143: loss did not improve\n",
      "Epoch 144/150\n",
      "163773/163773 [==============================] - 150s 913us/step - loss: 0.9326 - acc: 0.6993\n",
      "\n",
      "Epoch 00144: loss did not improve\n",
      "Epoch 145/150\n",
      "163773/163773 [==============================] - 150s 915us/step - loss: 0.9277 - acc: 0.7003\n",
      "\n",
      "Epoch 00145: loss improved from 0.93192 to 0.92766, saving model to weights-improvement-145-0.9277.hdf5\n",
      "Epoch 146/150\n",
      "163773/163773 [==============================] - 150s 915us/step - loss: 0.9306 - acc: 0.6993\n",
      "\n",
      "Epoch 00146: loss did not improve\n",
      "Epoch 147/150\n",
      "163773/163773 [==============================] - 150s 916us/step - loss: 0.9261 - acc: 0.7016\n",
      "\n",
      "Epoch 00147: loss improved from 0.92766 to 0.92613, saving model to weights-improvement-147-0.9261.hdf5\n",
      "Epoch 148/150\n",
      "163773/163773 [==============================] - 150s 915us/step - loss: 0.9214 - acc: 0.7027\n",
      "\n",
      "Epoch 00148: loss improved from 0.92613 to 0.92138, saving model to weights-improvement-148-0.9214.hdf5\n",
      "Epoch 149/150\n",
      "163773/163773 [==============================] - 151s 921us/step - loss: 0.9197 - acc: 0.7032\n",
      "\n",
      "Epoch 00149: loss improved from 0.92138 to 0.91968, saving model to weights-improvement-149-0.9197.hdf5\n",
      "Epoch 150/150\n",
      "163773/163773 [==============================] - 150s 916us/step - loss: 0.9194 - acc: 0.7031\n",
      "\n",
      "Epoch 00150: loss improved from 0.91968 to 0.91935, saving model to weights-improvement-150-0.9194.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Fit the model by specifying different parameters like number of epochs and batchsize\n",
    "\n",
    "history = model.fit(X, y, epochs=150, batch_size=128, callbacks=callbacks_list, shuffle=True).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights from the saved check point checkpoint\n",
    "\n",
    "filename = \"weights-improvement-150-0.9194.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "#convert the integers back to characters\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"rew sn hare ht sas to iar eredr saatid to see what tai aou po eneli hewe thet aotw dn oneh alrisr thie the hooseh in the aou. \\n'  aod toue t\",\n",
       " 'erland  aetcrsa wiire cli then she heard a little thing sar dnwn an ier htea    \\n                                                           ',\n",
       " \" cein a\\nmatdr kartin aeued wo cnw shoet inm to the lerghan thmiei taids a mocd turtle iare thet was pot at yhet? ata they dei't prahe arwn t\",\n",
       " \"or it was oney soe pom,' said alice to herself, 'i wonder what iatpen teids of the sooe, \\n'what iare yhuh thrien aro and aoy toa- saleen a l\",\n",
       " \"ed states without paying cnpyright holder found at the taml tia \\nor shecs'' said the daterpillar saimed tire to thet aele mo aste siress ffr\",\n",
       " 'he whnl oake yhsh hnra she sald thing as allce taids hntt a \\nor the blcie an see could not think of anything tounles that had falten sear sh',\n",
       " 'he nuoer and hos aetiean the telesteng iy al in aomiu whic \"     \\'                                                                          ',\n",
       " \", and the gatter. 'i dod't tei  wha kal oeas the donr aedin shen, and whnt bs she cadee \\n                                                   \",\n",
       " 'ded round her aeceme aoi oh the theegre oa whrneut farsies wo cn a tian tite the qreen aaris thete sas eoin anoy, seetedre taid anxnd sher w',\n",
       " \"eats cake, but at any rate i'sl netee sernugd thin ween oae wt fem    a iite herkll teihl hfad fown to her fer, \\nsoeeshrg the aledse the com\",\n",
       " \"ole uent she sioued ae fxuenn she had teis ditteabdr feipni whue the loak turtle said: 'attcpe celih thet sieeeled arpyhrg it whsh the lext \",\n",
       " \"lt ierery kert-'\\n alm      dre tha cuoiia toee ot ooead '' \\n\\n\\n                                                                              \",\n",
       " \"garden ano oi crene so the quoer as phes anl aor a pacn of cards!'\\n\\nat this the whole party swam to the searide once ou twige, see htwiioe w\",\n",
       " \"t fardent.'\\nanice ceutras thae so hare bn hersnlf th the queen said so the quoer as phes anl aor a pacn of cards!'\\n\\nat this the whole party \",\n",
       " \"ve! iln'te thas had falten sear she was so much aloeady sie sas oo a lote tab ay phe sinn carargn fo the monk of the crorted the crymt was\\ng\",\n",
       " 'om\\n     the prejnsusnine antnons, \\n\\n.ld whi wadke ia tal mi the monent ho whth the lext tian noesen hnsa ther aeiin, thes whne aeafnst the c',\n",
       " 'the look of it at all, at the largh hare whnt bnlss to ba ax saying to her uery well wou moah tar toflpinn io her hand ana a large pisto was',\n",
       " \" she had teis ditteabdr feipni whue the loak turtle said: 'attcpe celih thet sieeeled arpyhrg it whsh the lext tian noesen hnsa ther aeiin, \",\n",
       " \"rt wiry laae if aanit her so whik ste onrec,\\noh\\nlautna an surt as the door and foentiogs to aepuy the pooa  'wruld atiid. \\n'to, cndssidn ir \",\n",
       " \"yalty payments      mn kury         oo jvrrear oursar pealsrn, 'bn so yht\\nio the siate boe theie say elrn a can yat '\\nthi kad aouehdls fnl s\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting an input and giving out the output by looping for 20 times\n",
    "# Pick a random input from X\n",
    "# Predicting characterwise 140 times (length of a tweet)\n",
    "# Store the complete output (20 times 140 character strings) in a file\n",
    "\n",
    "file = []\n",
    "for i in range(20):\n",
    "\n",
    "    start = np.random.randint(0, len(dataX)-1)\n",
    "    pattern = dataX[start]\n",
    "    \n",
    "    line = ''\n",
    "    \n",
    "    for i in range(140):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(n_vocab)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        seq_in = [int_to_char[value] for value in pattern]\n",
    "        line += result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "    #print(\"\\n\")\n",
    "    #final += '\\n'\n",
    "    file.append(line)\n",
    "file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 and 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew sn hare ht sas to iar eredr saatid to see what tai aou po eneli hewe thet aotw dn oneh alrisr thie the hooseh in the aou. \n",
      "'  aod toue t\n",
      "erland  aetcrsa wiire cli then she heard a little thing sar dnwn an ier htea    \n",
      "                                                           \n",
      " cein a\n",
      "matdr kartin aeued wo cnw shoet inm to the lerghan thmiei taids a mocd turtle iare thet was pot at yhet? ata they dei't prahe arwn t\n",
      "or it was oney soe pom,' said alice to herself, 'i wonder what iatpen teids of the sooe, \n",
      "'what iare yhuh thrien aro and aoy toa- saleen a l\n",
      "ed states without paying cnpyright holder found at the taml tia \n",
      "or shecs'' said the daterpillar saimed tire to thet aele mo aste siress ffr\n",
      "he whnl oake yhsh hnra she sald thing as allce taids hntt a \n",
      "or the blcie an see could not think of anything tounles that had falten sear sh\n",
      "he nuoer and hos aetiean the telesteng iy al in aomiu whic \"     '                                                                          \n",
      ", and the gatter. 'i dod't tei  wha kal oeas the donr aedin shen, and whnt bs she cadee \n",
      "                                                   \n",
      "ded round her aeceme aoi oh the theegre oa whrneut farsies wo cn a tian tite the qreen aaris thete sas eoin anoy, seetedre taid anxnd sher w\n",
      "eats cake, but at any rate i'sl netee sernugd thin ween oae wt fem    a iite herkll teihl hfad fown to her fer, \n",
      "soeeshrg the aledse the com\n",
      "ole uent she sioued ae fxuenn she had teis ditteabdr feipni whue the loak turtle said: 'attcpe celih thet sieeeled arpyhrg it whsh the lext \n",
      "lt ierery kert-'\n",
      " alm      dre tha cuoiia toee ot ooead '' \n",
      "\n",
      "\n",
      "                                                                              \n",
      "garden ano oi crene so the quoer as phes anl aor a pacn of cards!'\n",
      "\n",
      "at this the whole party swam to the searide once ou twige, see htwiioe w\n",
      "t fardent.'\n",
      "anice ceutras thae so hare bn hersnlf th the queen said so the quoer as phes anl aor a pacn of cards!'\n",
      "\n",
      "at this the whole party \n",
      "ve! iln'te thas had falten sear she was so much aloeady sie sas oo a lote tab ay phe sinn carargn fo the monk of the crorted the crymt was\n",
      "g\n",
      "om\n",
      "     the prejnsusnine antnons, \n",
      "\n",
      ".ld whi wadke ia tal mi the monent ho whth the lext tian noesen hnsa ther aeiin, thes whne aeafnst the c\n",
      "the look of it at all, at the largh hare whnt bnlss to ba ax saying to her uery well wou moah tar toflpinn io her hand ana a large pisto was\n",
      " she had teis ditteabdr feipni whue the loak turtle said: 'attcpe celih thet sieeeled arpyhrg it whsh the lext tian noesen hnsa ther aeiin, \n",
      "rt wiry laae if aanit her so whik ste onrec,\n",
      "oh\n",
      "lautna an surt as the door and foentiogs to aepuy the pooa  'wruld atiid. \n",
      "'to, cndssidn ir \n",
      "yalty payments      mn kury         oo jvrrear oursar pealsrn, 'bn so yht\n",
      "io the siate boe theie say elrn a can yat '\n",
      "thi kad aouehdls fnl s\n"
     ]
    }
   ],
   "source": [
    "# Use tweetbot to tweet 20 times, 1 tweet per 6 mins (360 seconds)\n",
    "# The file created in the last step is used here and is tweeted line by line (20 times, 140 characters each)\n",
    "\n",
    "\n",
    "### NOTE -  The credentials for the Twitter Account have been removed\n",
    "# Please put in credentials before running this code\n",
    "\n",
    "import tweepy\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# Create variables for each key, secret, token\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "# Set up OAuth and integrate with API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Write a tweet to push to our Twitter account\n",
    "#tweet = 'This is Tweetbot tweeting!'\n",
    "#api.update_status(status=tweet)\n",
    "\n",
    "#print lines and tweet only if the line is not blank\n",
    "for line in file:\n",
    "    print(line)\n",
    "    if line != '\\n':\n",
    "        api.update_status(line)\n",
    "\n",
    "# Add an else statement with pass to conclude the conditional statement\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "# Add sleep method to space tweets by 5 seconds each\n",
    "    sleep(360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1972669f320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot Train Accuracies as a function of epochs\n",
    "\n",
    "plt.plot(history['acc'])\n",
    "#plt.plot(history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19726745358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot Train Losses as a function of epoch\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "#plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
